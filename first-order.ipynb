{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order\n",
    "\n",
    "This notebook was automatically generated from the Algorithms for Optimization source code. Each cell generates a figure from the original text. While this code is not optimized for use in lectures, we provide it here to be adapted for such projects. We hope you find it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"support_code.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\n",
    "\tp = let\n",
    "\t\tfunction secant_method(df, x1, x2, ϵ)\n",
    "\t\t    df1 = df(x1)\n",
    "\n",
    "\t\t    delta = Inf\n",
    "\t\t    while abs(delta) > ϵ\n",
    "\t\t    \tdf2 = df(x2)\n",
    "\t\t        delta = (x2 - x1)/(df2 - df1)*df2\n",
    "\t\t        x1, x2, df1 = x2, x2 - delta, df2\n",
    "\t\t    end\n",
    "\t\t    x2\n",
    "\t\tend\n",
    "\n",
    "\t\tf = (x,y) -> x^2 + y^2 + y*sin(x)\n",
    "\t\tdf = (x,y) -> [2x + y*cos(x), 2y + sin(x)]\n",
    "\n",
    "\t\txdomain = (-2, 4)\n",
    "\t\tydomain = (-2, 4)\n",
    "\n",
    "\t\tp0 = (-0.5, 3.5)\n",
    "\t\tpts = Tuple{Float64,Float64}[p0]\n",
    "\t\tfor i in 1 : 3\n",
    "\t\t\tx,y = pts[end]\n",
    "\t\t\tdp = df(x, y)\n",
    "\t\t\tvdp = VecE2{Float64}(-dp[1], -dp[2])\n",
    "\n",
    "\t\t\tf1d = a -> begin\n",
    "\t\t\t\tx2 = x + a*dp[1]\n",
    "\t\t\t\ty2 = y + a*dp[2]\n",
    "\n",
    "\t\t\t\tda = df(x2, y2)\n",
    "\t\t\t\tpa = VecE2{Float64}(da[1], da[2])\n",
    "\t\t\t\tproj(pa, vdp, Float64)\n",
    "\t\t\tend\n",
    "\t\t\talpha = secant_method(f1d, 0.0, 1.0, 0.001)\n",
    "\n",
    "\t\t\tpush!(pts, (x + alpha*dp[1], y + alpha*dp[2]))\n",
    "\t\tend\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=0:2:10, style=\"width=\\\\columnwidth\"))\n",
    "\t\tpush!(plots, Plots.Linear3([p[1] for p in pts], [p[2] for p in pts], [f(p[1], p[2]) for p in pts], style=\"black, solid, mark=*, mark size = 1, mark options={draw=black}\"))\n",
    "\n",
    "\t\tg = GroupPlot(2, 1)\n",
    "\t\tpush!(g, Axis(plots, height=\"5cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", zlabel=L\"y\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false\"))\n",
    "\t\tpush!(g, Axis(plots, width=\"5cm\", height=\"5cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, axis equal, view={0}{90}\"))\n",
    "\t\tg\n",
    "\tend\n",
    "\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\tusing LinearAlgebra\n",
    "\n",
    "\tpts_gradient_banana = Tuple{Float64,Float64}[]\n",
    "\n",
    "\tp = let\n",
    "\t\tfunction secant_method(df, x1, x2, ϵ)\n",
    "\t\t    df1 = df(x1)\n",
    "\n",
    "\t\t    delta = Inf\n",
    "\t\t    while abs(delta) > ϵ\n",
    "\t\t    \tdf2 = df(x2)\n",
    "\t\t        delta = (x2 - x1)/(df2 - df1)*df2\n",
    "\t\t        x1, x2, df1 = x2, x2 - delta, df2\n",
    "\t\t    end\n",
    "\t\t    x2\n",
    "\t\tend\n",
    "\n",
    "\n",
    "\t\tf = (x,y) -> (1-x)^2 + 5*(y - x^2)^2\n",
    "\t\tdf = (x,y) -> [2*(10*x^3-10*x*y+x-1), 10*(y-x^2)]\n",
    "\n",
    "\t\txdomain = (-2, 2)\n",
    "\t\tydomain = (-2, 2)\n",
    "\n",
    "\t\tp0 = (-1, -1)\n",
    "\t\tpts = Tuple{Float64,Float64}[p0]\n",
    "\t\tfor i in 1 : 10\n",
    "\t\t\tx,y = pts[end]\n",
    "\t\t\tdp = normalize(-VecE2{Float64}(df(x, y)...))\n",
    "\n",
    "\t\t\tf1d = a -> begin\n",
    "\t\t\t\tx2 = x + a*dp.x\n",
    "\t\t\t\ty2 = y + a*dp.y\n",
    "\n",
    "\t\t\t\tda = df(x2, y2)\n",
    "\t\t\t\tpa = VecE2{Float64}(da[1], da[2])\n",
    "\t\t\t\tproj(pa, dp, Float64)\n",
    "\t\t\tend\n",
    "\t\t\talpha = secant_method(f1d, 0.0, 1.0, 0.0001)\n",
    "\t\t\tpush!(pts, (x + alpha*dp.x, y + alpha*dp.y))\n",
    "\t\tend\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=[1,2,3,5,10,20,50,100], style=\"width=\\\\columnwidth\"))\n",
    "\t\tpush!(plots, Plots.Linear3([p[1] for p in pts], [p[2] for p in pts], [f(p[1], p[2]) for p in pts], style=\"black, solid, mark=none\"))\n",
    "\n",
    "\t\tappend!(pts_gradient_banana, pts)\n",
    "\n",
    "\t\tAxis(plots, width=\"8cm\", height=\"8cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, axis equal, view={0}{90}\")\n",
    "\tend\n",
    "\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type DescentMethod end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tp = let\n",
    "\n",
    "\t\tA = Float64[1 -0.9; -0.9 1]\n",
    "\t\tb = Float64[0, 0]\n",
    "\t\tf = x -> (x'*A*x)[1]\n",
    "\t\t∇f = x -> 2*A'*x\n",
    "\n",
    "\t\tx = Float64[-1.1, -1.8]\n",
    "\t\tpts = Vector{Float64}[x]\n",
    "\n",
    "\t\tfunction get_optimal_step_size(d, A, b)\n",
    "\t\t\treturn -(d'*(A*x+b))[1]/(d'*A*d)[1]\n",
    "\t\tend\n",
    "\n",
    "\t\t# FIRST ITERATION\n",
    "\t\td = -∇f(x)\n",
    "\t\tα = get_optimal_step_size(d, A, b)\n",
    "\t\tx += α*d\n",
    "\t\tpush!(pts, x)\n",
    "\n",
    "\t\t# SECOND ITERATION\n",
    "\t\tg = ∇f(x)\n",
    "\t\tβ = (g'*A*d)[1] / (d'*A*d)[1]\n",
    "\t\td = -g + β*d\n",
    "\t\tα = get_optimal_step_size(d, A, b)\n",
    "\t\tx += α*d\n",
    "\t\tpush!(pts, x)\n",
    "\n",
    "\t\txdomain = (-2, 2)\n",
    "\t\tydomain = (-2, 2)\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain))\n",
    "\t\tpush!(plots, Plots.Linear3([x[1] for x in pts],\n",
    "\t\t                           [x[2] for x in pts],\n",
    "\t\t                           [f(x) for x in pts], style=\"black, solid, mark=*, mark size = 1, mark options={draw=black}\"))\n",
    "\t\tpush!(plots, Plots.Node(L\"1\", pts[1][1], pts[1][2], style=\"right\"))\n",
    "\t\tpush!(plots, Plots.Node(L\"2\", pts[2][1], pts[2][2], style=\"left\"))\n",
    "\t\tpush!(plots, Plots.Node(L\"3\", pts[3][1], pts[3][2], style=\"right\"))\n",
    "\n",
    "\n",
    "\t\tAxis(plots, width=\"\\\\columnwidth\", height=\"\\\\columnwidth\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, axis equal, view={0}{90}\")\n",
    "\tend\n",
    "\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\n",
    "\tp = let\n",
    "\t\tfunction secant_method(df, x1, x2, ϵ)\n",
    "\t\t    df1 = df(x1)\n",
    "\n",
    "\t\t    delta = Inf\n",
    "\t\t    while abs(delta) > ϵ\n",
    "\t\t    \tdf2 = df(x2)\n",
    "\t\t        delta = (x2 - x1)/(df2 - df1)*df2\n",
    "\t\t        x1, x2, df1 = x2, x2 - delta, df2\n",
    "\t\t    end\n",
    "\t\t    x2\n",
    "\t\tend\n",
    "\n",
    "\t\tf = x -> (1-x[1])^2 + 5*(x[2] - x[1]^2)^2\n",
    "\t\tdf = x -> [2*(10*x[1]^3-10*x[1]*x[2]+x[1]-1), 10*(x[2]-x[1]^2)]\n",
    "\n",
    "\t\tx0 = Float64[-1, -1]\n",
    "\t\tpts = Vector{Float64}[x0]\n",
    "\t\tg0 = df(x0)\n",
    "\t\td0 = -g0\n",
    "\t\tf1d = a -> begin\n",
    "\t\t\tda = df(x0 + a*d0)\n",
    "\t\t\tproj(VecE2{Float64}(da[1], da[2]), VecE2{Float64}(d0[1], d0[2]), Float64)\n",
    "\t\tend\n",
    "\t\talpha = secant_method(f1d, 0.0, 0.1, 0.0001) # NOTE: is sensitive to the value of 0.1, does not work with 1.0!\n",
    "\t\tpush!(pts, x0 + alpha*d0)\n",
    "\n",
    "\t\tfor i in 1 : 5\n",
    "\t\t\tx = pts[end]\n",
    "\t\t\tg1 = df(x)\n",
    "\t\t\tβ = max(0.0, dot(g1, g1-g0)/dot(g0,g0))\n",
    "\t\t\td1 = -g1 + β*d0\n",
    "\n",
    "\t\t\tf1d = a -> begin\n",
    "\t\t\t\tda = df(x + a*d1)\n",
    "\t\t\t\tproj(VecE2{Float64}(da[1], da[2]), VecE2{Float64}(d1[1], d1[2]), Float64)\n",
    "\t\t\tend\n",
    "\t\t\talpha = secant_method(f1d, 0.0, 0.1, 0.0001)\n",
    "\t\t\tpush!(pts, x + alpha*d1)\n",
    "\n",
    "\t\t\td0 = d1\n",
    "\t\t\tg0 = g1\n",
    "\t\tend\n",
    "\n",
    "\t\txdomain = (-2, 2)\n",
    "\t\tydomain = (-2, 2)\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=[1,2,3,5,10,20,50,100], style=\"width=\\\\columnwidth\"))\n",
    "\t\tpush!(plots, Plots.Linear3([p[1] for p in pts_gradient_banana],\n",
    "\t\t                           [p[2] for p in pts_gradient_banana],\n",
    "\t\t                           [f([p[1], p[2]]) for p in pts_gradient_banana], style=\"gray, solid, mark=none\"))\n",
    "\t\tpush!(plots, Plots.Linear3([p[1] for p in pts], [p[2] for p in pts], [f(p) for p in pts], style=\"black, solid, mark=none\"))\n",
    "\n",
    "\n",
    "\t\tAxis(plots, width=\"8cm\", height=\"8cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, axis equal, view={0}{90}\")\n",
    "\tend\n",
    "\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\n",
    "\tstruct GradientDescent <: DescentMethod\n",
    "\t\tα\n",
    "\tend\n",
    "\tinit!(M::GradientDescent, f, ∇f, x) = M\n",
    "\tfunction step!(M::GradientDescent, f, ∇f, x)\n",
    "\t\tα, g = M.α, ∇f(x)\n",
    "\t\treturn x - α*g\n",
    "\tend\n",
    "\tmutable struct Momentum <: DescentMethod\n",
    "\t\tα # learning rate\n",
    "\t\tβ # momentum decay\n",
    "\t\tv # momentum\n",
    "\tend\n",
    "\tfunction init!(M::Momentum, f, ∇f, x)\n",
    "\t\tM.v = zeros(length(x))\n",
    "\t\treturn M\n",
    "\tend\n",
    "\tfunction step!(M::Momentum, f, ∇f, x)\n",
    "\t\tα, β, v, g = M.α, M.β, M.v, ∇f(x)\n",
    "\t\tv[:] = β*v - α*g\n",
    "\t\treturn x + v\n",
    "\tend\n",
    "\n",
    "\tp = let\n",
    "\t\tf = x -> (1-x[1])^2 + 100*(4x[2] - x[1]^2)^2\n",
    "\t\t∇f = x -> [2*(200x[1]^3 - 800x[1]*x[2] + x[1] - 1), -800*(x[1]^2 - 4x[2])]\n",
    "\n",
    "\t\txdomain = (-3, 2)\n",
    "\t\tydomain = (-0.5, 2)\n",
    "\n",
    "\t\tfunction this_step!(M::DescentMethod, v::VecE2{Float64})\n",
    "\t\t    x = Float64[v.x, v.y]\n",
    "\t\t    return VecE2{Float64}(step!(M, f, ∇f, x)...)\n",
    "\t\tend\n",
    "\t\tfunction run_descent_method(M::DescentMethod, x₀::VecE2{Float64}, N::Int)\n",
    "\t\t    pts = [x₀]\n",
    "\t\t    init!(M, f, ∇f, Float64[x₀.x, x₀.y])\n",
    "\t\t    for i in 1 : N\n",
    "\t\t        push!(pts, this_step!(M, pts[end]))\n",
    "\t\t    end\n",
    "\t\t    return pts\n",
    "\t\tend\n",
    "\t\tfunction get_descent_plot(pts::Vector{VecE2{Float64}}, name, color::String=\"black\")\n",
    "\t\t    Plots.Linear([P.x for P in pts], [P.y for P in pts], style=\"thick, $color, solid, mark=none, line join=round\", legendentry=name) #$\n",
    "\t\tend\n",
    "\n",
    "\t\tx₀ = VecE2{Float64}(-2,1.5)\n",
    "\t\tN = 40\n",
    "\n",
    "\t\tstuff = Tuple{DescentMethod, String, String}[]\n",
    "\t\tpush!(stuff, (GradientDescent(0.0003), \"gradient descent\", \"pastelRed\"))\n",
    "\t\tpush!(stuff, (Momentum(0.0003, 0.9, zeros(2)), \"momentum\", \"black\"))\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=[2,10,50,200,500], style=\"forget plot, width=\\\\columnwidth\", xbins=150, ybins=150))\n",
    "\t\tfor (M, name, color) in stuff\n",
    "\t\t    pts = run_descent_method(M, x₀, N)\n",
    "\t\t    push!(plots, get_descent_plot(pts, name, color))\n",
    "\t\tend\n",
    "\t\tAxis(plots, width=\"12cm\", height=\"6cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, view={0}{90}, legend pos=outer north east\")\n",
    "\tend\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\n",
    "\tmutable struct NesterovMomentum <: DescentMethod\n",
    "\t\tα # learning rate\n",
    "\t\tβ # momentum decay\n",
    "\t\tv # momentum\n",
    "\tend\n",
    "\tfunction init!(M::NesterovMomentum, f, ∇f, x)\n",
    "\t\tM.v = zeros(length(x))\n",
    "\t\treturn M\n",
    "\tend\n",
    "\tfunction step!(M::NesterovMomentum, f, ∇f, x)\n",
    "\t\tα, β, v = M.α, M.β, M.v\n",
    "\t\tv[:] = β*v - α*∇f(x + β*v)\n",
    "\t\treturn x + v\n",
    "\tend\n",
    "\n",
    "\tp = let\n",
    "\t\tf = x -> (1-x[1])^2 + 100*(4x[2] - x[1]^2)^2\n",
    "\t\t∇f = x -> [2*(200x[1]^3 - 800x[1]*x[2] + x[1] - 1), -800*(x[1]^2 - 4x[2])]\n",
    "\n",
    "\t\txdomain = (-3, 2)\n",
    "\t\tydomain = (-0.5, 2)\n",
    "\n",
    "\t\tfunction this_step!(M::DescentMethod, v::VecE2{Float64})\n",
    "\t\t    x = convert(Vector{Float64}, v)\n",
    "\t\t    return VecE2{Float64}(step!(M, f, ∇f, x))\n",
    "\t\tend\n",
    "\t\tfunction run_descent_method(M::DescentMethod, x₀::VecE2{Float64}, N::Int)\n",
    "\t\t    pts = [x₀]\n",
    "\t\t    init!(M, f, ∇f, convert(Vector{Float64}, x₀))\n",
    "\t\t    for i in 1 : N\n",
    "\t\t        push!(pts, this_step!(M, pts[end]))\n",
    "\t\t    end\n",
    "\t\t    return pts\n",
    "\t\tend\n",
    "\t\tfunction get_descent_plot(pts::Vector{VecE2{Float64}}, name, color::String=\"black\")\n",
    "\t\t    Plots.Linear([P.x for P in pts], [P.y for P in pts], style=\"thick, $color, solid, mark=none, line join=round\", legendentry=name) #$\n",
    "\t\tend\n",
    "\n",
    "\t\tx₀ = VecE2{Float64}(-2,1.5)\n",
    "\t\tdm = GradientDescent(0.0003)\n",
    "\t\tN = 40\n",
    "\n",
    "\t\tstuff = Tuple{DescentMethod, String, String}[]\n",
    "\t\t# push!(stuff, (GradientDescent(0.0003), \"gradient descent\", \"black\"))\n",
    "\t\tpush!(stuff, (Momentum(0.0003, 0.9, zeros(2)), \"momentum\", \"black\"))\n",
    "\t\tpush!(stuff, (NesterovMomentum(0.0002, 0.92, zeros(2)), \"Nesterov momentum\", \"pastelRed\"))\n",
    "\t\t# push!(stuff, (Adagrad(0.1, 1e-8, zeros(2)), \"Adagrad\", \"green\"))\n",
    "\t\t# push!(stuff, (RMSProp(0.65, 0.45, 1e-8, zeros(2)), \"RMSProp\", \"green\"))\n",
    "\t\t# push!(stuff, (Adadelta(0.8, 0.8, 1e-3, zeros(2), zeros(2)), \"Adadelta\", \"orange\"))\n",
    "\t\t# push!(stuff, (Adam(0.8, 0.9, 0.9, 1e-8, 0, zeros(2), zeros(2)), \"Adam\", \"purple\"))\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=[2,10,50,200,500], style=\"forget plot, width=\\\\columnwidth\", xbins=150, ybins=150))\n",
    "\t\tfor (M, name, color) in stuff\n",
    "\t\t    pts = run_descent_method(M, x₀, N)\n",
    "\t\t    push!(plots, get_descent_plot(pts, name, color))\n",
    "\t\tend\n",
    "\t\tAxis(plots, width=\"12cm\", height=\"6cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, view={0}{90}, legend pos=outer north east\")\n",
    "\tend\n",
    "\tplot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tusing Vec\n",
    "\n",
    "\tmutable struct HyperGradientDescent <: DescentMethod\n",
    "\t\tα0 # initial learning rate\n",
    "\t\tμ # learning rate of the learning rate\n",
    "\t\tα # current learning rate\n",
    "\t\tg_prev # previous gradient\n",
    "\tend\n",
    "\tfunction init!(M::HyperGradientDescent, f, ∇f, x)\n",
    "\t\tM.α = M.α0\n",
    "\t\tM.g_prev = zeros(length(x))\n",
    "\t\treturn M\n",
    "\tend\n",
    "\tfunction step!(M::HyperGradientDescent, f, ∇f, x)\n",
    "\t\tα, μ, g, g_prev = M.α, M.μ, ∇f(x), M.g_prev\n",
    "\t\tα = α + μ*(g⋅g_prev)\n",
    "\t\tM.g_prev, M.α = g, α\n",
    "\t\treturn x - α*g\n",
    "\tend\n",
    "\n",
    "\tmutable struct HyperNesterovMomentum <: DescentMethod\n",
    "\t\tα0 # initial learning rate\n",
    "\t\tμ # learning rate of the learning rate\n",
    "\t\tβ # momentum decay\n",
    "\t\tv # momentum\n",
    "\t\tα # current learning rate\n",
    "\t\tg_prev # previous gradient\n",
    "\tend\n",
    "\tfunction init!(M::HyperNesterovMomentum, f, ∇f, x)\n",
    "\t\tM.α = M.α0\n",
    "\t\tM.v = zeros(length(x))\n",
    "\t\tM.g_prev = zeros(length(x))\n",
    "\t\treturn M\n",
    "\tend\n",
    "\tfunction step!(M::HyperNesterovMomentum, f, ∇f, x)\n",
    "\t\tα, β, μ = M.α, M.β, M.μ\n",
    "\t\tv, g, g_prev = M.v, ∇f(x), M.g_prev\n",
    "\t\tα = α - μ*(g⋅(-g_prev - β*v))\n",
    "\t\tv[:] = β*v + g\n",
    "\t\tM.g_prev, M.α = g, α\n",
    "\t\treturn x - α*(g + β*v)\n",
    "\tend\n",
    "\n",
    "\tp = let\n",
    "\t\tf = x -> (1-x[1])^2 + 100*(4x[2] - x[1]^2)^2\n",
    "\t\t∇f = x -> [2*(200x[1]^3 - 800x[1]*x[2] + x[1] - 1), -800*(x[1]^2 - 4x[2])]\n",
    "\n",
    "\t\txdomain = (-3, 2)\n",
    "\t\tydomain = (-0.5, 2)\n",
    "\n",
    "\t\tfunction this_step!(M::DescentMethod, v::VecE2{Float64})\n",
    "\t\t    x = convert(Vector{Float64}, v)\n",
    "\t\t    return VecE2{Float64}(step!(M, f, ∇f, x))\n",
    "\t\tend\n",
    "\t\tfunction run_descent_method(M::DescentMethod, x₀::VecE2{Float64}, N::Int)\n",
    "\t\t    pts = [x₀]\n",
    "\t\t    init!(M, f, ∇f, convert(Vector{Float64}, x₀))\n",
    "\t\t    for i in 1 : N\n",
    "\t\t        push!(pts, this_step!(M, pts[end]))\n",
    "\t\t    end\n",
    "\t\t    return pts\n",
    "\t\tend\n",
    "\t\tfunction get_descent_plot(pts::Vector{VecE2{Float64}}, name, color::String=\"black\")\n",
    "\t\t    Plots.Linear([P.x for P in pts], [P.y for P in pts], style=\"thick, $color, solid, mark=none, line join=round\", legendentry=name) #$\n",
    "\t\tend\n",
    "\n",
    "\t\tx₀ = VecE2{Float64}(-2,1.5)\n",
    "\t\tdm = GradientDescent(0.0003)\n",
    "\t\tN = 40\n",
    "\n",
    "\t\tstuff = Tuple{DescentMethod, String, String}[]\n",
    "\t\tpush!(stuff, (HyperGradientDescent(0.0004, 8e-13, NaN, zeros(2)), \"hypermomentum\", \"black\"))\n",
    "\t\tpush!(stuff, (HyperNesterovMomentum(0.00023, 1e-12, 0.93, zeros(2), NaN, zeros(2)), \"hyper-Nesterov\", \"pastelRed\"))\n",
    "\n",
    "\t\tplots = Plots.Plot[]\n",
    "\t\tpush!(plots, Plots.Contour(f, xdomain, ydomain, levels=[2,10,50,200,500], style=\"forget plot, width=\\\\columnwidth\", xbins=150, ybins=150))\n",
    "\t\tfor (M, name, color) in stuff\n",
    "\t\t    pts = run_descent_method(M, x₀, N)\n",
    "\t\t    push!(plots, get_descent_plot(pts, name, color))\n",
    "\t\tend\n",
    "\t\tAxis(plots, width=\"12cm\", height=\"6cm\", xlabel=L\"x_1\", ylabel=L\"x_2\", style=\"xtick=\\\\empty, ytick=\\\\empty, contour/labels=false, view={0}{90}, legend pos=outer north east\")\n",
    "\tend\n",
    "\tplot(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
